{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубинное обучение для текстовых данных, ФКН ВШЭ\n",
    "\n",
    "## Домашнее задание 1: Text Suggestion\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — 10 баллов. Сдавать задание после жесткого дедлайна нельзя. При сдачи решения после мягкого дедлайна за каждый день просрочки снимается по одному баллу.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "__Мягкий дедлайн: 02.10.24 23:59__\n",
    "\n",
    "__Жесткий дедлайн: 05.10.24 23:59__\n",
    "\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит реализовать систему, предлагающую удачное продолжение слова или нескольких следующих слов в режиме реального времени по типу тех, которые используются в телефонах, поисковой строке или приложении почты. Полученную систему вам нужно будет обернуть в пользовательский интерфейс с помощью библиотеки [reflex](https://github.com/reflex-dev/reflex), чтобы ей можно было удобно пользоваться, а так же, чтобы убедиться, что все работает как надо. В этот раз вам не придется обучать никаких моделей, мы ограничимся n-граммной генерацией.\n",
    "\n",
    "### Структура\n",
    "\n",
    "Это домашнее задание состоит из двух частей предположительно одинаковых по сложности. В первой вам нужно будет выполнить 5 заданий, по итогам которых вы получите минимально рабочее решение. А во второй, пользуясь тем, что вы уже сделали реализовать полноценную систему подсказки текста с пользовательским интерфейсом. Во второй части мы никак не будем ограничивать вашу фантазию. Делайте что угодно, лишь бы получилось в результате получился удобный фреймворк. Чем лучше у вас будет результат, тем больше баллов вы получите. Если будет совсем хорошо, то мы добавим бонусов сверху по своему усмотрению.\n",
    "\n",
    "### Оценивание\n",
    "При сдаче зададания в anytask вам будет необходимо сдать весь код, а также отчет с подробным описанием техник, которые в применили для создания вашей системы. Не лишним будет также написать и о том, что у вас не получилось и почему.\n",
    "\n",
    "За часть с заданиями можно будет получить до __5__ баллов, за отчет – до __3__ баллов и еще __2__ балла можно будет получить за демонстрацию вашей системы и пользовательского интерфейса. Демонстрацию прикрепляйте в anytask в виде 1-2 минутной записи экрана."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные\n",
    "\n",
    "Для получения текстовых статистик используйте датасет `emails.csv`. Вы можете найти его по [ссылке](https://disk.yandex.ru/d/ikyUhWPlvfXxCg). Он содержит более 500 тысяч электронных писем на английском языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517401"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emails = pd.read_csv('emails.csv')\n",
    "len(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметьте, что данные очень грязные. В каждом письме содержится различная мета-информация, которая будет только мешать при предсказании продолжения текста.\n",
    "\n",
    "__Задание 1 (1 балл).__ Очистите корпус текстов по вашему усмотрению. В идеале обработанные тексты должны содержать только текст самого письма и ничего лишнего по типу ссылок, адресатов и прочих символов, которыми мы точно не хотим продолжать текст. Оценка будет выставляться по близости вашего результата к этому идеалу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import email\n",
    "from email import policy\n",
    "\n",
    "def get_email_body_from_forward_part(email_content):\n",
    "    headers_pattern = re.compile(r\"Subject:\\s*(.*?)\\r?\\n\")\n",
    "\n",
    "    match = headers_pattern.search(email_content)\n",
    "\n",
    "    if not match:\n",
    "        return email_content\n",
    "\n",
    "    body_start = match.end()\n",
    "\n",
    "    return email_content[body_start:].strip()\n",
    "\n",
    "\n",
    "def extract_email_body(email_text):\n",
    "    msg = email.message_from_string(email_text, policy=policy.default)\n",
    "    \n",
    "    if msg.is_multipart():\n",
    "        for part in msg.walk():\n",
    "            if part.get_content_type() == 'text/plain':\n",
    "                encoding = part.get_content_charset() or \"utf-8\"\n",
    "\n",
    "                return part.get_payload(decode=True).decode(encoding).lower()\n",
    "    encoding = msg.get_content_charset() or \"utf-8\"\n",
    "\n",
    "    text = msg.get_payload(decode=True).decode(encoding)\n",
    "\n",
    "    several_emails = None\n",
    "    if \"----- Forwarded by\" in text:\n",
    "        several_emails = \"----- Forwarded by\"\n",
    "    elif \"-----Original Message\" in text:\n",
    "        several_emails = \"-----Original Message\"\n",
    "    if several_emails:\n",
    "        forwarded_bodies = []\n",
    "        forwarded_parts = text.split(several_emails)\n",
    "\n",
    "        for part in forwarded_parts[1:]:\n",
    "            forwarded_bodies.append(get_email_body_from_forward_part(part))\n",
    "        \n",
    "        return re.sub(r'[^\\S ]+', ' ', \" \".join(forwarded_bodies).lower())\n",
    "    \n",
    "    return re.sub(r'[^\\S ]+', ' ', text.lower())\n",
    "\n",
    "emails['cleaned_message'] = emails['message'].apply(extract_email_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для следующего задания вам нужно будет токенизировать текст. Для этого просто разбейте его по словам. Очевидно, итоговый результат будет лучше, если ваша система также будет предлагать уместную пунктуацию. Но если вы считаете, что результат получается лучше без нее, то можете удалить все небуквенные символы на этапе токенизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "class WhitespaceOutput:\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.ids = None\n",
    "\n",
    "\n",
    "class WhitespaceTokenizer:\n",
    "    def __init__(self, corpus, vocab_size=-1, n=2):\n",
    "        self.n = n\n",
    "        self.regex = re.compile(r'\\w+|[^\\w\\s]+')\n",
    "\n",
    "        words = []\n",
    "        for line in tqdm(corpus):\n",
    "            words.extend(self.regex.findall(line))\n",
    "        \n",
    "        word_count = Counter(words)\n",
    "        if vocab_size == -1:\n",
    "            self.vocab = set(word_count)\n",
    "        else:\n",
    "            common = word_count.most_common(vocab_size - 3)\n",
    "            self.vocab, _ = zip(*common)\n",
    "            self.vocab = set(self.vocab)\n",
    "\n",
    "        self.vocab |= set(['[UNK]', '[BOS]', '[EOS]'])\n",
    "    \n",
    "    def encode(self, text):\n",
    "        words = self.regex.findall(text)\n",
    "        encoded = [w.lower() if w.lower() in self.vocab else '[UNK]' for w in words]\n",
    "\n",
    "        encoded = ['[BOS]'] * self.n + encoded + ['[EOS]']\n",
    "        return WhitespaceOutput(encoded)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "    \n",
    "\n",
    "def get_tokenizer(corpus, vocab_size=32768, n=2):\n",
    "    return WhitespaceTokenizer(corpus, vocab_size, n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнение слова\n",
    "\n",
    "Описанная система будет состоять из двух частей: дополнение слова до целого и генерация продолжения текста (или вариантов продолжений). Начнем с первой части.\n",
    "\n",
    "В этой части вам предстоит реализовать метод дополнения слова до целого по его началу (префиксу). Для этого сперва необходимо научиться находить все слова, имеющие определенный префикс. Мы будем вызывать функцию поиска подходящих слов после каждой напечатанной пользователем буквы. Поэтому нам очень важно, чтобы поиск работал как можно быстрее. Простой перебор всех слов занимает $O(|V| \\cdot n)$ времени, где $|V|$ – размер словаря, а $n$ – длина префикса. Мы же напишем [префиксное дерево](https://ru.wikipedia.org/wiki/Префиксное_дерево), которое позволяет искать слова за $O(n + m)$, где $m$ – число подходящих слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2 (1 балл).__ Допишите префиксное дерево для поиска слов по префиксу. Ваше дерево должно работать за $O(n + m)$ операции, в противном случае вы не получите баллов за это задание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class PrefixTreeNode:\n",
    "    def __init__(self):\n",
    "        # словарь с буквами, которые могут идти после данной вершины\n",
    "        self.children: dict[str, PrefixTreeNode] = {}\n",
    "        self.is_end_of_word = False\n",
    "\n",
    "class PrefixTree:\n",
    "    def __init__(self, vocabulary: List[str]):\n",
    "        \"\"\"\n",
    "        vocabulary: список всех уникальных токенов в корпусе\n",
    "        \"\"\"\n",
    "        self.root = PrefixTreeNode()\n",
    "        \n",
    "        for word in vocabulary:\n",
    "            self._insert_word(word)\n",
    "    \n",
    "    def _insert_word(self, word: str):\n",
    "        current_node = self.root\n",
    "        for letter in word:\n",
    "            if letter not in current_node.children:\n",
    "                current_node.children[letter] = PrefixTreeNode()\n",
    "            current_node = current_node.children[letter]\n",
    "\n",
    "        current_node.is_end_of_word = True\n",
    "\n",
    "    def search_prefix(self, prefix) -> List[str]:\n",
    "        \"\"\"\n",
    "        Возвращает все слова, начинающиеся на prefix\n",
    "        prefix: str – префикс слова\n",
    "        \"\"\"\n",
    "\n",
    "        current_node = self.root\n",
    "\n",
    "        for letter in prefix:\n",
    "            if letter not in current_node.children:\n",
    "                return []\n",
    "            current_node = current_node.children[letter]\n",
    "\n",
    "        return self._dfs(current_node, prefix)\n",
    "\n",
    "    def _dfs(self, node: PrefixTreeNode, prefix: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Выполняет поиск всех слов, начиная с текущего узла в дереве.\n",
    "        \"\"\"\n",
    "        words = []\n",
    "\n",
    "        if node.is_end_of_word:\n",
    "            words.append(prefix)\n",
    "        \n",
    "        for letter, next_node in node.children.items():\n",
    "            words.extend(self._dfs(next_node, prefix + letter))\n",
    "        \n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = ['aa', 'aaa', 'abb', 'bba', 'bbb', 'bcd']\n",
    "prefix_tree = PrefixTree(vocabulary)\n",
    "\n",
    "assert set(prefix_tree.search_prefix('a')) == set(['aa', 'aaa', 'abb'])\n",
    "assert set(prefix_tree.search_prefix('bb')) == set(['bba', 'bbb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда у нас есть способ быстро находить все слова с определенным префиксом, нам нужно их упорядочить по вероятности, чтобы выбирать лучшее. Будем оценивать вероятность слова по частоте его встречаемости в корпусе.\n",
    "\n",
    "__Задание 3 (1 балл).__ Допишите класс `WordCompletor`, который формирует словарь и префиксное дерево, а так же умеет находить все возможные продолжения слова вместе с их вероятностями. В этом классе вы можете при необходимости дополнительно отфильтровать слова, например, удалив все самые редкие. Постарайтесь максимально оптимизировать ваш код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "class WordCompletor:\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"\n",
    "        corpus: list – корпус текстов\n",
    "        \"\"\"\n",
    "        self.words_count = Counter(list(chain.from_iterable(corpus)))\n",
    "        self.prefix_tree = PrefixTree(list(self.words_count.keys()))\n",
    "\n",
    "    def get_words_and_probs(self, prefix: str) -> (List[str], List[float]):\n",
    "        \"\"\"\n",
    "        Возвращает список слов, начинающихся на prefix,\n",
    "        с их вероятностями (нормировать ничего не нужно)\n",
    "        \"\"\"\n",
    "        words, probs = [], []\n",
    "        \n",
    "        words = self.prefix_tree.search_prefix(prefix.lower())\n",
    "\n",
    "        probs = [self.words_count[word] / sum(self.words_count.values()) for word in words]\n",
    "\n",
    "        return words, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "    [\"aa\", \"ab\"],\n",
    "    [\"aaa\", \"abab\"],\n",
    "    [\"abb\", \"aa\", \"ab\", \"bba\", \"bbb\", \"bcd\"],\n",
    "]\n",
    "\n",
    "word_completor = WordCompletor(dummy_corpus)\n",
    "words, probs = word_completor.get_words_and_probs('a')\n",
    "words_probs = list(zip(words, probs))\n",
    "assert set(words_probs) == {('aa', 0.2), ('ab', 0.2), ('aaa', 0.1), ('abab', 0.1), ('abb', 0.1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание следующих слов\n",
    "\n",
    "Теперь, когда мы умеем дописывать слово за пользователем, мы можем пойти дальше и предожить ему несколько следующих слов с учетом дописанного. Для этого мы воспользуемся n-граммами и будем советовать n следующих слов. Но сперва нужно получить n-граммную модель.\n",
    "\n",
    "Напомним, что вероятность последовательности для такой модели записывается по формуле\n",
    "$$\n",
    "P(w_1, \\dots, w_T) = \\prod_{i=1}^T P(w_i \\mid w_{i-1}, \\dots, w_{i-n}).\n",
    "$$\n",
    "\n",
    "Тогда, нам нужно оценить $P(w_i \\mid w_{i-1}, \\dots, w_{i-n})$ по частоте встречаемости n-граммы.   \n",
    "\n",
    "__Задание 4 (1 балл).__ Напишите класс для n-граммной модели. Понятное дело, никакого сглаживания добавлять не надо, мы же не хотим, чтобы модель советовала случайные слова (хоть и очень редко)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, corpus, n, tokenizer: WhitespaceTokenizer | None = None):\n",
    "        self.n = n\n",
    "\n",
    "        self.counts = self._count_ngrams(corpus)\n",
    "\n",
    "        self.prefix_counts = dict()\n",
    "        for prefix, token_count in self.counts.items():\n",
    "            self.prefix_counts[prefix] = sum(token_count.values())\n",
    "\n",
    "    def _count_ngrams(self, corpus: list[list[str]] | list[str]) -> dict[str, Counter]:\n",
    "        counts = defaultdict(Counter)\n",
    "\n",
    "        for text in tqdm(corpus, desc=\"Counting Ngrams\"):\n",
    "            for i in range(self.n, len(text)):\n",
    "                prefix = tuple(text[i - self.n:i])\n",
    "                token = text[i]\n",
    "                counts[prefix][token] += 1\n",
    "            \n",
    "        return counts\n",
    "\n",
    "    def get_next_words_and_probs(self, prefix: list[str] | str) -> tuple[List[str], List[float]]:\n",
    "        \"\"\"\n",
    "        Возвращает список слов, которые могут идти после prefix,\n",
    "        а так же список вероятностей этих слов\n",
    "        \"\"\"\n",
    "\n",
    "        next_words, probs = [], []\n",
    "        \n",
    "        prefix = self._get_last_n_words(prefix)\n",
    "\n",
    "        possible_tokens = self.counts[prefix]\n",
    "\n",
    "        if len(possible_tokens) == 0:\n",
    "            return ['[EOS]'], [1.0]\n",
    "\n",
    "        next_words = list(possible_tokens.keys())\n",
    "\n",
    "        counts = list(possible_tokens.values())\n",
    "\n",
    "        probs = np.array(counts) / self.prefix_counts[prefix]\n",
    "\n",
    "        return next_words, probs\n",
    "    \n",
    "    def _get_last_n_words(self, prefix: list[str] | str) -> tuple[str]:\n",
    "        if isinstance(prefix, str):\n",
    "            prefix = self.tokenizer.encode(prefix).tokens[:-1]\n",
    "        \n",
    "        return tuple(prefix[-self.n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Ngrams: 100%|██████████| 3/3 [00:00<00:00, 25890.77it/s]\n"
     ]
    }
   ],
   "source": [
    "dummy_corpus = [\n",
    "    ['aa', 'aa', 'aa', 'aa', 'ab'],\n",
    "    ['aaa', 'abab'],\n",
    "    ['abb', 'aa', 'ab', 'bba', 'bbb', 'bcd']\n",
    "]\n",
    "\n",
    "n_gram_model = NGramLanguageModel(corpus=dummy_corpus, n=2)\n",
    "\n",
    "next_words, probs = n_gram_model.get_next_words_and_probs(['aa', 'aa'])\n",
    "words_probs = list(zip(next_words, probs))\n",
    "\n",
    "assert set(words_probs) == {('aa', 2/3), ('ab', 1/3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, мы теперь можем объединить два метода в автоматический дописыватель текстов: первый будет дополнять слово, а второй – предлагать продолжения. Хочется, чтобы предлагался список возможных продолжений, из который пользователь сможет выбрать наиболее подходящее. Самое сложное тут – аккуратно выбирать, что показывать, а что нет.   \n",
    "\n",
    "__Задание 5 (1 балл).__ В качестве первого подхода к снаряду реализуйте метод, возвращающий всегда самое вероятное продолжение жадным способом. Если вы справитесь, то сможете можете добавить опцию поддержки нескольких вариантов продолжений, что сделает метод гораздо лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSuggestion:\n",
    "    def __init__(self, word_completor: WordCompletor, n_gram_model: NGramLanguageModel):\n",
    "        self.word_completor = word_completor\n",
    "        self.n_gram_model = n_gram_model\n",
    "\n",
    "    def suggest_text(self, text: str | list, n_words=3, n_texts=1) -> list[list[str]]:\n",
    "        \"\"\"\n",
    "        Возвращает возможные варианты продолжения текста (по умолчанию только один)\n",
    "        \n",
    "        text: строка или список слов – написанный пользователем текст\n",
    "        n_words: число слов, которые дописывает n-граммная модель\n",
    "        n_texts: число возвращаемых продолжений (пока что только одно)\n",
    "        \n",
    "        return: list[list[srt]] – список из n_texts списков слов, по 1 + n_words слов в каждом\n",
    "        Первое слово – это то, которое WordCompletor дополнил до целого.\n",
    "        \"\"\"\n",
    "\n",
    "        suggestions = []\n",
    "\n",
    "        if isinstance(text, str):\n",
    "            text = text.split()\n",
    "        \n",
    "        for _ in range(n_texts):\n",
    "            completions, completions_probs = self.word_completor.get_words_and_probs(text[-1])\n",
    "            most_probable_completion = completions[max(enumerate(completions_probs), key=lambda x: x[1])[0]]\n",
    "            text_copy = text.copy()\n",
    "            suggestion = [most_probable_completion]\n",
    "            for _ in range(n_words):\n",
    "                next_words, next_words_probs = self.n_gram_model.get_next_words_and_probs(text_copy)\n",
    "                most_probable_next_word = next_words[max(enumerate(next_words_probs), key=lambda x: x[1])[0]]\n",
    "                suggestion.append(most_probable_next_word)\n",
    "                text_copy.append(most_probable_next_word)\n",
    "\n",
    "            suggestions.append(suggestion)\n",
    "\n",
    "        return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Ngrams: 100%|██████████| 3/3 [00:00<00:00, 78643.20it/s]\n"
     ]
    }
   ],
   "source": [
    "dummy_corpus = [\n",
    "    ['aa', 'aa', 'aa', 'aa', 'ab'],\n",
    "    ['aaa', 'abab'],\n",
    "    ['abb', 'aa', 'ab', 'bba', 'bbb', 'bcd']\n",
    "]\n",
    "\n",
    "word_completor = WordCompletor(dummy_corpus)\n",
    "n_gram_model = NGramLanguageModel(corpus=dummy_corpus, n=2)\n",
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)\n",
    "\n",
    "assert text_suggestion.suggest_text(['aa', 'aa'], n_words=3, n_texts=1) == [['aa', 'aa', 'aa', 'aa']]\n",
    "assert text_suggestion.suggest_text(['abb', 'aa', 'ab'], n_words=2, n_texts=1) == [['ab', 'bba', 'bbb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 517401/517401 [00:20<00:00, 25141.71it/s]\n"
     ]
    }
   ],
   "source": [
    "all_cleaned_emails = emails['cleaned_message'].values\n",
    "tokenizer = get_tokenizer(all_cleaned_emails, 65536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 517401/517401 [00:40<00:00, 12810.45it/s]\n"
     ]
    }
   ],
   "source": [
    "emails_corpus = [tokenizer.encode(clean_email).tokens for clean_email in tqdm(emails['cleaned_message'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Ngrams: 100%|██████████| 517401/517401 [01:54<00:00, 4524.40it/s] \n"
     ]
    }
   ],
   "source": [
    "word_completor_emails = WordCompletor(emails_corpus)\n",
    "n_gram_model_email = NGramLanguageModel(corpus=emails_corpus, n=2)\n",
    "text_suggestion = TextSuggestion(word_completor_emails, n_gram_model_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewut/Documents/nlp/hw1/nlp_venv/lib/python3.11/site-packages/dill/_dill.py:422: PicklingWarning: Cannot locate reference to <class '__main__.WhitespaceTokenizer'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/Users/andrewut/Documents/nlp/hw1/nlp_venv/lib/python3.11/site-packages/dill/_dill.py:422: PicklingWarning: Cannot locate reference to <class '__main__.TextSuggestion'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/Users/andrewut/Documents/nlp/hw1/nlp_venv/lib/python3.11/site-packages/dill/_dill.py:422: PicklingWarning: Cannot locate reference to <class '__main__.WordCompletor'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/Users/andrewut/Documents/nlp/hw1/nlp_venv/lib/python3.11/site-packages/dill/_dill.py:422: PicklingWarning: Cannot locate reference to <class '__main__.PrefixTree'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/Users/andrewut/Documents/nlp/hw1/nlp_venv/lib/python3.11/site-packages/dill/_dill.py:422: PicklingWarning: Cannot locate reference to <class '__main__.PrefixTreeNode'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/Users/andrewut/Documents/nlp/hw1/nlp_venv/lib/python3.11/site-packages/dill/_dill.py:422: PicklingWarning: Cannot locate reference to <class '__main__.NGramLanguageModel'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import sys\n",
    "\n",
    "sys.setrecursionlimit(100000)\n",
    "\n",
    "with open('tokenizer.pkl', 'wb') as file:\n",
    "    dill.dump(tokenizer, file, byref=True)\n",
    "\n",
    "with open('text_suggestion_em.pkl', 'wb') as file:\n",
    "    dill.dump(text_suggestion, file, byref=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настало время довести вашу систему до ума. В этой части вы можете модифицировать все классы по своему усмотрению и добавлять любые эвристики. Если нужно, то дополнительно обрабатывать текст и вообще делать все, что считаете нужным, __кроме использования дополнительных данных__. Главное – вы должны обернуть вашу систему в пользовательский интерфейс с помощью [reflex](https://github.com/reflex-dev/reflex). В нем можно реализовать почти любой функционал по вашему желанию.\n",
    "\n",
    "Мы настоятельно рекомендуем вам оформить код в проект, а не писать в ноутбуке. Но если вам очень хочется писать тут, то хотя бы не меняйте код в предыдущих заданиях, чтобы его можно было нормально оценивать.\n",
    "\n",
    "При сдаче решения прикрепите весь ваш __код__, __отчет__ по второй части и __видео__ с демонстрацией работы вашей системы. Удачи!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
  "notebookPath": "seminar.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
